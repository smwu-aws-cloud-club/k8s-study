## 네임스페이스

- 단일 클러스터 내에서 리소스 그룹 격리 매커니즘 제공 → 네임스페이스를 사용하면 리소스를 그룹화 하여 사용할 수 있음
- 네임스페이스는 디플로이먼트, 서비스 등 네임스페이스 기반 오브젝트에서만 구분 가능하며 클러스터 범위의 오브젝트에서는 적용 불가
- 쿠버네티스 에서는 같은 이름의 리소스라도 서로 다른 namespace에 존재할 수 있음
    - my-app이라는 Pod가 dev/prod 네임스페이스에 동시에 존재 가능
    
    ```yaml
    kubectl get pod my-app -n dev
    kubectl get pod my-app -n prod
    ```
    
- 네임스페이스의 필요성
    - 여러 네임스페이스를 사용하여 많은 구성 요소를 가진 복잡한 시스템을 좀 더 작은 개별 그룹으로 분리할 수 있음
    - 리소스를 프로덕션, 개발 ,QA 환경 등 원하는 방향으로 커스텀해서 사용 가능

### 초기 네임스페이스

- 쿠버네티스의 default namespace
- default
    - 네임스페이스를 먼저 생성하지 않고도 새 클러스터 사용 가능
- kube-node-lease
    - kubelet이 heartbeat를 보내 control plane이 node의 장애 탐지 가능
- kube-public
    - 모든 client가 읽기 권한으로 접근 가능 → 공개되어 읽을 수 있는 리소스에 적용
- kube-system
    - 쿠버네티스 시스템에서 생성한 오브젝트를 위한 네임스페이스

### 명령어

```bash
kubectl get namespace #cluster에 있는 모든 네임스페이스 확인

kubectl get pods --namespace kube-system #kube-system namespace에 해당하는 pods 확인
  
```

- 네임스페이스 생성
    - yaml
    
    ```bash
    apiVersion: v1
    kind: Namsepace
    metadata:
    	name: my-namespace
    ```
    
    - command line
    
    ```bash
    kubectl create namespace my-namespace
    ```
    

## 오토스케일링

- 쿠버네티스에서 HorizontalPodAutoscaler는 워크로드 리소스를 자동으로 업데이트하며, 워크로드의 크기를 수요에 맞게 자동으로 스케일링
- 예측할 수 없는 트래픽 증가에 대해 자동으로 확장 가능
- 수평 스케일링 → 부하 증가에 대비해 파드를 더 배치
- 부하량이 줄어들고 파드의 수가 설정값 이상인 경우 HorizontalPodAutoscaler는 워크로드 리소스(디플로이먼트, 스테이트풀셋, 기타 리소스)에게 스케일 다운 지시
- HorizontalPodAutoscaler는 크기 조절이 불가능한 오브젝트(데몬셋)에는 적용할 수 없음
- HorizontalPodAutoscaler는 쿠버네티스 API 리소스 및 컨트롤러 형태로 구현되어 있음 → 쿠버네티스 control plane 내에서 실행되는 HPA 컨트롤러는 평균 CPU 사용률, 평균 메모리 사용률, 메트릭 등을 감지하여 리소스를 조정

### 수평적 파드 오토스케일링

- 컨트롤러가 관리하는 Pods의 레플리카 수를 자동으로 조정하는 것
- HPA controller에 의해 수행됨
- HPA를 작성하여 활성화시키고 원하는 대로 설정
- 컨트롤러는 주기적으로 파드 메트릭 확인 → HPA 리소스에 설정돼 있는 대상 메트릭 값을 만족하는 레플리카 수를 계산 → 대상 리소스(디플로이먼트, 레플리카셋, 레플리케이션 컨트롤러, 스테이트풀셋) 안에 있는 replicas field 값을 조절

### 오토스케일링 프로세스

- 확장 가능한 리소스 오브젝트에서 관리하는 모든 파드의 메트릭을 가져오면 → 메트릭을 지정한 목표 값과 같거나 가깝도록 하기 위해 필요한 파드 수를 계산함 → 확장 가능한 리소스의 replicas field를 갱신
- HPA 컨트롤러는 원하는 메트릭 값과 현재 메트릭 값 사이의 비율로 동작함

### 롤링 업데이트 중 오토스케일링

- 디플로이먼트에 대한 롤링 업데이트 시 디플로이먼트는 레플리카셋을 관리
- 디플로이먼트에 오토스케일링을 설정하려면, 각 디플로이먼트에 대한 HPA 생성 필요
- HPA는 디플로이먼트의 replicas의 필드 관리

### API 오브젝트

- HPA API 오브젝트 생성 시 지정된 이름이 유효한 DNS 서브도메인 이름인지 확인해야 함

### 리소스 메트릭 지원

- 모든 HPA 대상은 스케일링 대상에서 파드의 리소스(cpu,memory) 사용량을 기준으로 스케일링 할 수 있음
- 이 리소스 요청을 가지고 HPA 컨트롤러는 스케일링 대상을 관리

### cpu 사용률 기반 스케일링

- CPU가 완전히 바쁜 상태에 도달하기 전에 스케일 아웃(파드 수 증가)을 수행하는 것이 좋음
- 오토스케일링이 필요한 파드는 LimitRange 오브젝트를 통해 cpu요청을 설정해야 함
- cpu 사용량을 기반으로 HPA 생성
    - 디플로이먼트 작성 후 cpu 리소스 요청 정의
    
    ```bash
    apiVersion: extensions/vlbeta1
    kind: Deployment 
    	metadata:
    		name: kubia 
    	spec:
    		replicas: 3 
    		template:
    		metadata: 
    			name: kubia
    		labels:
    			app: kubia
    	spec: 
    		containers:
    		- image: luksa/kubia:v1 
    			name: nodejs 
    			resources: #pod당 100밀리코어의 cpu 요청(cpu리소스 요청 정의)
    				requests: 
    					cpu: 10m
    ```
    
    - 해당 디플로이먼트를 생성한 다음 HPA 오브젝트를 생성해 앞에서 생성한 디플로이먼트를 가리키도록 함
        
        ```bash
        kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5
        ```
        
        - HPA 오브젝트를 생성하고, kubia 디플로이먼트를 스케일링 대상으로 설정
        - 파드의 목표 cpu 사용량을 30%로 지정하고 최소(1) 및 최대 레플리카 수(5) 지정
        - 오토스케일러는 cpu 사용률을 30%대로 유지하기 위해 레플리카 수를 계속 조정하지만 1개 미만으로 줄이거나 5개를 초과하는 레플리카를 만들지는 않음
